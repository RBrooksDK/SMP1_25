{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Stochastic Modelling and Processes - Spring 2025 <p>Repository for SMP1-S25 at VIA.</p> <p>Checkout the homepage!</p> </p> <p> </p>"},{"location":"#course-information","title":"Course information","text":"<ul> <li>Course responsible: Associate Professor Richard Brooks, rib@via.dk</li> <li>5 ECTS (European Credit Transfer System), corresponding to 130 hours of work</li> <li>10 sessions, each with a duration of 4 lessons, starting in week 6</li> <li>Bachelor level course - the course is academically challenging working on problems independently.</li> <li>Grade: 7-step scale</li> <li>Type of assessment: 4 hours written exam (see exam description in the menu to the left)</li> <li>Recommended prerequisites: In the session plan below, a dedicated entry is made for prerequisites.</li> </ul>"},{"location":"#lectures-and-course-organization","title":"Lectures and course organization","text":"<p>The course is scheduled to start in week 6 and will be held on Tuesdays from 12:45 to 16:05. In general, each session is made up of four activities:</p> <ol> <li>At the beginning of each session, there will be a short recap of the previous session.</li> <li>We then go through the exercises from the previous session.</li> <li>We will go through the theory of the current session.</li> <li>After classes, and before the next session, you will have to solve exercises from the current session.</li> </ol> <p>This then loops back to (1) at the beginning of the next session.</p> <p>There are no mandatory assignments, but it is highly recommended work on the exercises for each session. No instruction is provided for the exercises so you will have to work on them on your own or form study groups.</p>"},{"location":"#course-content-and-learning-objectives","title":"Course content and learning objectives","text":"<p>Stochastic Modelling and Processes is the art of making sense of randomness in the world around us. We examine probability theory, finding the tools to describe and analyse random systems mathematically. You'll learn about random variables \u2014 their mean, variance, and the distributions that define them \u2014 and learn how these concepts power everything from decision-making to machine learning.</p> <p>Learning Objectives</p> <ul> <li>Probability: Understand the fundamental concepts of probability theory, including experiments, sample spaces, independence, conditional probability, and Bayes' theorem. Learn to approach random systems methodically using probabilistic reasoning.</li> <li>Random Variables: Describe and analyse random systems through random variables. Understand their characteristics, including mean, variance, standard deviation, and commonly used distributions like normal, binomial, and Poisson.</li> <li>Point Estimation: Learn techniques to estimate population parameters from sample data and evaluate the quality and reliability of these estimates.</li> <li>Statistical Intervals: Construct and interpret confidence intervals for population parameters. Learn to assess the precision of estimates and their implications for statistical inference.</li> <li>Hypothesis Testing: Explore the principles of hypothesis testing. Learn to formulate null and alternative hypotheses, compute and interpret p-values, and make informed decisions based on statistical evidence.</li> <li>Regression Analysis: Investigate relationships between variables using regression models. Understand how to fit, interpret, and assess the quality of regression models for real-world data.</li> <li>Stochastic Processes: Model and analyse systems that evolve over time using stochastic processes, including applications of Markov Chains for dynamic systems.</li> <li>Python for Statistical Modelling: Gain hands-on experience with Python for data analysis, simulating random variables, conducting statistical tests, and visualizing statistical data to reinforce theoretical understanding.</li> <li>Critique and Evaluate Statistical Models: Develop the competence to critically assess statistical models and results. Identify sources of error, critique experimental designs, and propose improvements for better reliability.</li> </ul> <p>But it's not just about theory. You'll get hands-on with Python, simulating randomness, running statistical tests, and exploring applications of stochastic models. By the end, you\u2019ll not only understand how to model uncertainty but also how to use it to make informed predictions and decisions.</p>"},{"location":"#resources","title":"Resources","text":"<p>ASPE: Montgomery, D.C. &amp; Runger, G.C.. Applied Statistics and Probability for Engineers, 7<sup>th</sup> edition. All references are to chapters or exercises (found in the end of the book). Solutions to all exercises from the book are uploaded. You need to retrieve a copy however you usually retrieve books.</p> <p>Non-session specific resources such as the exercises from the book, solutions, old exam cases, etc. can be found her:</p> <p>General Resources SMP</p> <p>Wiseflow code for all flows that are used during the course is always 0000. This is not the code for the actual exam in June, though.</p> <p>The course is loosely built up around H. Pishro-Nik's https://www.probabilitycourse.com/</p> <p>I have compiled and uploaded all session from January 2021 to youtube. The link below will take you to a playlist containing all 10 sessions (theory only)</p> <p>Stochastic Modelling 2021 \u2013 All sessions</p> <p>Make sure you install a working version of Jupyter Notebook and Python version 3.7 or higher. The easiest way to install Python and Jupyter is using Anaconda Distribution. You can choose whichever framework you want to work in as long as it can handle Jupyter Notebooks. Installing VS Code with a Jupyter Notebook extension seems to be a popular choice.</p>"},{"location":"#historical-notes","title":"Historical Notes","text":"<p>Stichastic Modelling and Processing was first offered in 2014 and has been scheduled 1-2 times per year since then. The course responsible is Richard Brooks (RIB) and has been the only lecturer teaching the course.</p> <p>Grade Distribution 2024 (ordinary exam only)</p> Grade Count 12 1 10 8 7 12 4 5 02 3 00 2 -3 3"},{"location":"00_Important_Recap/","title":"00 Prerequisties","text":"Important Math Recap"},{"location":"00_Important_Recap/#material-for-the-math-recap","title":"Material for the math recap","text":"<p>Session material</p>"},{"location":"00_Important_Recap/#topics-to-recap","title":"Topics to recap","text":"<p>Boolean Algebra:</p> <ul> <li>Similar to sets so all rules also apply for sets</li> <li>Addition is same as union, multiplication the same as intersection and negation same as complement</li> <li>Important: Slide 12 about identities</li> </ul> <p>Sets</p> <ul> <li>Basic understanding of sets</li> <li>Important: Equivalence slide 29</li> </ul> <p>Combinatorics</p> <ul> <li>All rules are important and I assume you know them</li> <li>The most used one is combinations on slide 12</li> <li>Important: Formulas Slide 12</li> </ul> <p>Probability 1 + 2 (2.3\u00a0= Probability 2 slide 3):</p> <ul> <li>All rules of probability</li> <li>Important: Union 1.13, Conditional 2.2, Independence 2.3, Intersection 2.6, Law of Total Probability 2.8, Bayes\u2019 Theorem 2.10, Contingency Table 2.12, Joint Probability 2.14</li> </ul> <p>Sequences and Summation:</p> <ul> <li>The important part is Series from slide 19 onwards, not so much sequences (slides 1-18)</li> <li>Convergence slide 23</li> <li>Closed form formula for series slide 24</li> <li>Uselful summations and closed form slide 25</li> <li>Changing limits slide 26</li> </ul> <p>In Lesson 3, you will need to brush up on integrals.\u00a0It seems that Khan Academy have some nice tutorials on integrals that you may want to check out\u00a0this,\u00a0this,\u00a0and\u00a0this.</p> <p>In Lesson 9, you will need a little bit of Linear Algebra. I recommend reading \u201cMatrix Algebra\u201d which is the chapter we use from the Linear Algebra course. If you already passed\u00a0the Linear Algebra course (i.e. you are on your 7<sup>th</sup> semester), you will probably not need to worry about this prerequisite.</p>"},{"location":"01_Introduction_%2B_Recap_Probability_%2B_Stochastic_Variables/","title":"01 Introduction to Probability and Random Variables","text":"01 Introduction + Probability + Stochastic Variables"},{"location":"01_Introduction_%2B_Recap_Probability_%2B_Stochastic_Variables/#material","title":"Material:","text":"<p>ASPE: 1-3</p> <p>Session Notes</p> <p>Session material</p> <p>Make sure you are acquainted with common Series, Approximations, and Identities</p> <p>Session from 20/21: SMP 1</p>"},{"location":"01_Introduction_%2B_Recap_Probability_%2B_Stochastic_Variables/#topics","title":"Topics","text":"<p>Introduction - Why do we need statistics and probability theory? - Samples and populations - Measures - Scales of Measurement - Random Experiments &amp; Probabilities</p> <p>Probability (mostly recap) - Types of probability - Conditional probability - Conditional independence</p> <p>Discrete Stochastic Variables - Random variables - Probability Mass Function - Cumulative Distribution Functions - Independent random variables - Special Distributions - Expectation and Variance - Functions of Random Variables</p>"},{"location":"01_Introduction_%2B_Recap_Probability_%2B_Stochastic_Variables/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<ul> <li>All exercises in \u201cProblems 1\u201d but the most important are 5-7 with regard to exam cases. Also do:</li> <li>Exam 2014.2 (a+b)</li> <li>Exam 2015.2</li> <li>Re-Exam 2015.2 (a, c, d)</li> <li>Exam 2016 New Test.3</li> <li>Re-exam 2016.3</li> </ul>"},{"location":"02_Discrete_Random_Variables/","title":"02 Discrete Random Variables","text":"02 Discrete Random Variables"},{"location":"02_Discrete_Random_Variables/#material","title":"Material:","text":"<p>ASPE: 3</p> <p>Recap and Exercises - notes</p> <p>Session notes</p> <p>Session material</p> <p>Session from 20/21: SMP 2</p>"},{"location":"02_Discrete_Random_Variables/#topics","title":"Topics","text":"<ul> <li>Bernoulli Distribution</li> <li>Geometric Distribution</li> <li>Binomial Distribution</li> <li>Negative Binomial (Pascal) Distribution</li> <li>Hypergeometric Distribution</li> <li>Poisson Distribution</li> <li>Poisson as an approximation for binomial</li> <li>Cumulative Distribution Function</li> <li>Expectation and Variance</li> </ul>"},{"location":"02_Discrete_Random_Variables/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<ul> <li>All exercises in \u201cProblems 2\u201d ```python while student is \"bored\":     additional_exercises = [         \"Exam 2014.3 (a-c)\",         \"Exam 2016 New Test.2\",         \"Exam 2018.2\"     ]</li> </ul>"},{"location":"03_Continuous_Random_Variables/","title":"03 Continuous Random Variables","text":"03 Continuous Random Variables"},{"location":"03_Continuous_Random_Variables/#material","title":"Material:","text":"<p>ASPE: 4 (not 4.8-4.11)</p> <p>Recap notes</p> <p>Session notes Part 1</p> <p>Session notes Part 2</p> <p>Session material</p> <p>Session from 20/21: SMP 3 Today you will need to have brushed up on your basic calculus skills as this will not be recapped in depth. It seems that Khan Academy has some nice tutorials on integrals that you may want to check out: - Tutorial 1 - Tutorial 2 - Tutorial 3</p>"},{"location":"03_Continuous_Random_Variables/#topics","title":"Topics","text":"<p>Continuous random variables are a type of random variable that can take on an infinite number of values within a given range. The most important elements of continuous random variables are the probability density function, the cumulative distribution function, and the expected value. The probability density function provides the probability that a continuous random variable will take on a value within a given range, while the cumulative distribution function gives the probability that the variable will take on a value less than or equal to a given value. The expected value, or mean, of a continuous random variable is calculated by integrating the product of the variable and its probability density function over the entire range of possible values. The most important topics of today are:</p> <ul> <li>The probability density function</li> <li>The cumulative distribution function</li> <li>The normal distribution</li> <li>The exponential distribution</li> </ul> <p>It is also important that you think about the relationship between the discrete case and the continuous case, and how this transfers to the relationship between sums and integrals.</p>"},{"location":"03_Continuous_Random_Variables/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<p>(Make sure you did all of the exercises in Discrete Distributions)</p> <ul> <li>Exam 2014.1 + 3 (d+e)</li> <li>Exam 2018.1</li> <li>Reexam 2018.1</li> <li>Exam 2020.1</li> </ul>"},{"location":"04_Multivariate_Random_Variables/","title":"04 Multivariate Random Variables","text":"04 Multivariate Random Variables"},{"location":"04_Multivariate_Random_Variables/#material","title":"Material:","text":"<p>ASPE: 5 (not 5.5-5.8)</p> <p>Recap Notes</p> <p>CRVs Part 2 Notes</p> <p>Session notes</p> <p>Session material</p> <p>Session from 20/21: SMP 4</p>"},{"location":"04_Multivariate_Random_Variables/#topics","title":"Topics","text":"<p>This is arguably the most difficult of all the topics.</p> <p>Multivariate random variables are a collection of random variables that are correlated with each other. The most important elements of multivariate random variables include their mean vector, covariance matrix, and joint probability density function. The mean vector represents the average value of each variable, while the covariance matrix reflects the degree of correlation and variability among the variables. The joint probability density function specifies the probability of obtaining a particular combination of values for the variables. Understanding the key elements of multivariate random variables is crucial for performing statistical analyses, making predictions, and making decisions based on data.</p> <ul> <li>Joint Probability Distributions</li> <li>Conditional Probability Distributions</li> <li>Conditional Expectation</li> <li>Covariance and Correlation</li> </ul>"},{"location":"04_Multivariate_Random_Variables/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<p>Do Problems 4 - of special interest are 3-7, 1-2 are warm-up!</p>"},{"location":"05_Point_Estimation_and_sampling/","title":"05 Point Estimation and Sampling","text":"05 Point Estimation and Sampling"},{"location":"05_Point_Estimation_and_sampling/#material","title":"Material:","text":"<p>ASPE: 7 (not 7.3.4 and 7.4.3)</p> <p>Recap Exercises</p> <p>Session material</p> <p>Session from 20/21: SMP 5</p>"},{"location":"05_Point_Estimation_and_sampling/#topics","title":"Topics","text":"<p>You will most likely experience that the next 2-3 topics are a bit less complex than the previous three topics.</p> <p>Point estimation and sampling are important concepts in statistics. Point estimation involves using a statistic, such as the sample mean or proportion, to estimate an unknown population parameter. The goal is to find the value of the statistic that is most likely to be the true value of the parameter. Sampling, on the other hand, involves selecting a subset of individuals from a larger population and using their data to make inferences about the entire population. The key to effective sampling is ensuring that the sample is representative of the population and that the sample size is large enough to accurately estimate the population parameter of interest. Together, point estimation and sampling provide a framework for making accurate and reliable statistical inferences.</p> <ul> <li>Correlation and Covariance (from last week\u2019s topic)</li> <li>Sampling Distribution</li> <li>Central Limit Theorem</li> <li>Standard Error</li> <li>Mean Squared Error</li> <li>Maximum Likelihood Estimator</li> </ul>"},{"location":"05_Point_Estimation_and_sampling/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<ul> <li>ASPE: 7.2.8 + 7.2.10 + 7.3.3 + 7.3.9 + 7.4.4. (not b) + 7S13</li> <li>Also do Problems 5.</li> </ul>"},{"location":"06_Statistical_Intervals/","title":"06 Statistical Intervals","text":"06 Statistical Intervals"},{"location":"06_Statistical_Intervals/#material","title":"Material:","text":"<p>ASPE: 8 (not 8.6)</p> <p>Recap notes</p> <p>Session Notes</p> <p>Session material</p> <p>Session from 20/21: SMP 6</p>"},{"location":"06_Statistical_Intervals/#topics","title":"Topics","text":"<p>Statistical intervals are a way of expressing the uncertainty associated with a statistical estimate. They provide a range of values within which the true value of a population parameter or a future observation is likely to fall, based on a sample of data. Confidence intervals estimate the value of a population parameter with a measure of uncertainty, while prediction intervals provide a range of values for a future observation, taking into account both the uncertainty associated with estimating the population parameter and the variability associated with predicting individual observations. Statistical intervals are commonly used in various fields to estimate population parameters or future outcomes with a measure of precision and reliability.</p> <ul> <li>CI for mean</li> <li>CI for proportion</li> <li>CI for variance</li> <li>Tolerance and prediction interval</li> </ul>"},{"location":"06_Statistical_Intervals/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<p>Do Problems 6.</p>"},{"location":"07_Hypothesis_Testing/","title":"07 Hypothesis Testing","text":"07 Hypothesis Testing"},{"location":"07_Hypothesis_Testing/#material","title":"Material:","text":"<p>ASPE: 9.1-9.3 + 9.5 + 9.8 + 10.1-10.4 + 10.6</p> <p>Notes recap</p> <p>Session notes</p> <p>Session material</p> <p>Session from 20/21: SMP 7</p>"},{"location":"07_Hypothesis_Testing/#topics","title":"Topics","text":"<p>Hypothesis testing is a statistical method used to evaluate whether a certain hypothesis about a population parameter is supported by the data. The key elements of hypothesis testing include the formulation of a null hypothesis and an alternative hypothesis, the selection of an appropriate test statistic, the determination of a significance level or alpha value, the calculation of a p-value, and the comparison of the p-value to the significance level to decide whether to reject or fail to reject the null hypothesis. The significance level is a pre-determined threshold that represents the maximum probability of observing the data if the null hypothesis is true, and the p-value is the probability of observing the data, or more extreme data, if the null hypothesis is true. Hypothesis testing is widely used in many fields to make inferences about population parameters based on sample data, and it is an essential tool for scientific research and decision-making.</p> <ul> <li>Basics of hypothesis testing</li> <li>Type I and II errors</li> <li>P-values and critical values and test statistic</li> <li>Tests on mean and proportion</li> <li>One and two tailed tests</li> <li>Paired t-test</li> <li>Contingency table tests (next time in recap)</li> </ul>"},{"location":"07_Hypothesis_Testing/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<ul> <li>ASPE: 9.3.8 + 9.3.9 + 9.5.3 + 9.5.4</li> <li>Exam 2014.2.c; 2015.3; 2015.4; 2016.4; 2017.5; Reexam 2018.4 (not g)</li> </ul>"},{"location":"08_Regression/","title":"08 Regression","text":"08 Regression"},{"location":"08_Regression/#material","title":"Material:","text":"<p>ASPE: 11 (not 11.9)</p> <p>Recap notes</p> <p>Session notes</p> <p>Session material</p> <p>Session from 20/21: SMP 8</p>"},{"location":"08_Regression/#topics","title":"Topics","text":"<p>Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The key elements of linear regression include the dependent variable, independent variable(s), regression coefficients, intercept, and residual error. The regression coefficients represent the change in the dependent variable associated with a one-unit change in the independent variable, while the intercept represents the expected value of the dependent variable when all independent variables are equal to zero. The residual error is the difference between the observed values of the dependent variable and the values predicted by the regression model. The goal of linear regression is to find the best-fitting line that minimizes the sum of the squared residual errors. Linear regression is a widely used method in various fields to make predictions or estimate the strength and direction of relationships between variables.</p> <ul> <li>Least Squares Estimates</li> <li>Regression Equation</li> <li>Correlation</li> <li>Coefficient of determination</li> <li>Prediction</li> <li>Residual Analysis</li> </ul> <p>I've added a page with all the useful formulas for calculating the parameters as well as the performance metrics, \\(r\\) and \\(r^2\\)</p>"},{"location":"08_Regression/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<p>If nothing is noted, assume it is \u201cExam\u201d. If it is the reexam, it will be stated.</p> <ul> <li>2020.4 (not f), 2020.5, 2020.6, 2020.7, Reexam 2018.4, Reexam 2018.5, Reexam 2018.6, 2018.6, 2017.4, 2017.5, 2017.6</li> </ul>"},{"location":"08_Regression/Calculating%20metrics/","title":"Calculating Metrics in Simple Linear Regression","text":""},{"location":"08_Regression/Calculating%20metrics/#the-slope-and-intercept","title":"The Slope (and intercept)","text":"<p>In simple linear regression, the slope parameter (often denoted as \\(\\beta_1\\)) represents the relationship between the independent variable (X) and the dependent variable (Y). The slope indicates how much the dependent variable is expected to increase (or decrease) for a one-unit increase in the independent variable. There are several ways to compute this slope, and here are some of the key methods:</p>"},{"location":"08_Regression/Calculating%20metrics/#least-squares-estimation","title":"Least Squares Estimation:","text":"<p>The most common method for calculating the slope in simple linear regression is the least squares estimation. The formula for the slope (\\(\\beta_1\\)) using this method is:</p> <p>$$\\boxed{    \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n (x_i - \\overline{x})^2}}    $$</p> <p>In this expression: - \\(\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\) is the sum of the products of the deviations of \\(x\\) and \\(y\\) from their respective means. This term captures the covariance between \\(x\\) and \\(y\\), indicating how much \\(x\\) and \\(y\\) vary together from their mean values (though excluding \\(\\frac{1}{n-1}\\)). This term is often denoted \\(S_{xy}\\). - \\(\\overline{x}\\) and \\(\\overline{y}\\) are the means of \\(x\\) and \\(y\\), respectively. These are calculated as \\(\\overline{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) and \\(\\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_i\\), where \\(n\\) is the number of observations. - \\(\\sum_{i=1}^n (x_i - \\overline{x})^2\\) represents the sum of the squares of the deviations of \\(x\\) from its mean. This term is a measure of the total variance in \\(x\\) and helps normalize the covariance in the numerator. This term is often denoted \\(S_{xy}\\). - \\(n\\) is the number of observations, indicating the total number of data points in the dataset.</p>"},{"location":"08_Regression/Calculating%20metrics/#sum-of-products","title":"Sum of Products","text":"<p>The fundamental expression for the slope is the one stated above. But we can also use the sum of products which is demonstrated here.</p> <p>Derivation of \\((S_{xy})\\)</p> <p>The regular form for the sum of products of deviations for \\(x\\) and \\(y\\) is: $$ S_{xy} = \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) $$</p> <p>Expanding this sum: $$ S_{xy} = \\sum_{i=1}^n (x_i y_i - x_i \\overline{y} - y_i \\overline{x} + \\overline{x} \\overline{y}) $$</p> <p>Simplifying this by distributing the summation: $$ S_{xy} = \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\overline{y} - \\sum_{i=1}^n y_i \\overline{x} + n \\overline{x} \\overline{y} $$</p> <p>Notice that $\\sum_{i=1}^n x_i \\overline{y} $ can be rewritten because \\(\\overline{y}\\) is a constant: $$ \\sum_{i=1}^n x_i \\overline{y} = \\overline{y} \\sum_{i=1}^n x_i $$ And similarly for \\(\\sum_{i=1}^n y_i \\overline{x}\\): $$ \\sum_{i=1}^n y_i \\overline{x} = \\overline{x} \\sum_{i=1}^n y_i $$</p> <p>Since \\(\\sum_{i=1}^n x_i = n \\overline{x}\\) and \\(\\sum_{i=1}^n y_i = n \\overline{y}\\), the terms simplify to:  $$ S_{xy} = \\sum_{i=1}^n x_i y_i - n \\overline{x} \\overline{y} $$</p> <p>Derivation of \\(S_{xx}\\)</p> <p>The regular form for the sum of squares of deviations for $ x $ is: $$ S_{xx} = \\sum_{i=1}^n (x_i - \\overline{x})^2 $$</p> <p>Expanding this sum: $$ S_{xx} = \\sum_{i=1}^n (x_i^2 - 2x_i \\overline{x} + \\overline{x}^2) $$</p> <p>Simplifying this by distributing the summation: $$ S_{xx} = \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + n\\overline{x}^2 $$</p> <p>Since \\(\\sum_{i=1}^n x_i = n\\overline{x}\\): $$ S_{xx} = \\sum_{i=1}^n x_i^2 - 2n\\overline{x}^2 + n\\overline{x}^2 $$</p> <p>Combining the terms results in:  $$ S_{xx} = \\sum_{i=1}^n x_i^2 - n\\overline{x}^2 $$</p> <p>These derivations provide a clear mathematical pathway from the traditional definitions of $ S_{xy} $ and $ S_{xx} $ to the forms that are easily implemented in a python script for instance, and also means we can also formulate the slope as:</p> <p>$$\\boxed{ \\beta_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i=1}^n x_i^2 - n\\overline{x}^2}} $$</p>"},{"location":"08_Regression/Calculating%20metrics/#breakdown-using-individual-sum-terms","title":"Breakdown Using Individual Sum Terms","text":"<p>Another way to express this is by explicitly calculating the sums:</p> \\[\\boxed{ \\beta_1=\\frac{n \\sum\\left(x_i y_i\\right)-\\sum x_i \\sum y_i}{n \\sum\\left(x_i^2\\right)-\\left(\\sum x_i\\right)^2}} \\] <p>In this expression: - \\(n\\) is the number of observations., - \\(\\sum\\left(x_i y_i\\right)\\) is the sum of the products of corresponding \\(x\\) and \\(y\\) values, - \\(\\sum x_i\\) and \\(\\sum y_i\\) are the sums of \\(x\\) and \\(y\\) values, respectively, - \\(\\sum\\left(x_i^2\\right)\\) is the sum of the squares of \\(x\\) values.</p>"},{"location":"08_Regression/Calculating%20metrics/#covariance-and-variance-method","title":"Covariance and Variance Method:","text":"<p>This is essentially a rearrangement of the least squares formula, emphasizing the use of covariance and variance:    $$\\boxed{    \\beta_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}}    $$    where: - \\(\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{n-1}\\) (assuming a sample covariance) - \\(\\text{Var}(X) = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n-1}\\) (also assuming sample variance)</p>"},{"location":"08_Regression/Calculating%20metrics/#matrix-algebra-using-normal-equation","title":"Matrix Algebra (Using Normal Equation):","text":"<p>When dealing with linear regression in matrix terms, the slope can be calculated using the normal equation:    $$ \\boxed{    \\beta = (X^T X)^{-1} X^T Y}    $$    Here, \\(X\\) is the matrix of input features (including a column of ones for the intercept if it's included in the model), and \\(Y\\) is the vector of output values, and \\(\\beta\\) is a vector that will contain all the coefficients.</p>"},{"location":"08_Regression/Calculating%20metrics/#gradient-descent","title":"Gradient Descent:","text":"<p>Though not a formula in the traditional sense, gradient descent is an algorithmic approach used to find the minimum of the cost function (typically mean squared error) in regression. The update rule in each iteration for \\(\\beta_1\\) would be:    $$\\boxed{    \\beta_1^{(new)} = \\beta_1^{(old)} - \\alpha \\frac{\\partial}{\\partial \\beta_1} MSE}    $$    where \\(\\alpha\\) is the learning rate and \\(\\frac{\\partial}{\\partial \\beta_1} MSE\\) is the derivative of the mean squared error with respect to \\(\\beta_1\\).</p>"},{"location":"08_Regression/Calculating%20metrics/#regression-line-intercept-inclusion","title":"Regression Line Intercept Inclusion","text":"<p>We will also explain how the slope relates to the intercept in the regression equation, presented here as part of the full regression formula:</p> \\[ y = \\beta_0 + \\beta_1 x \\] <p>where $$ \\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n (x_i - \\overline{x})^2} $$ (or one of the alternatives from above) and $$\\boxed{ \\beta_0 = \\overline{y} - \\beta_1 \\overline{x}} $$</p>"},{"location":"08_Regression/Calculating%20metrics/#correlation-coefficient-r-and-correlation-of-determination-r2","title":"Correlation Coefficient (\\(r\\)) and Correlation of Determination (\\(r^2\\))","text":"<p>This section will present different formulations of \\(r\\) or \\(r^2\\). Depending on the formulation, one or the other is shown.</p>"},{"location":"08_Regression/Calculating%20metrics/#pearson-correlation-coefficient-r","title":"Pearson Correlation Coefficient (\\(r\\))","text":"<p>The Pearson correlation coefficient (\\(r\\)) measures the linear correlation between two variables, \\(x\\) and \\(y\\). It is defined as the ratio of the covariance of the variables to the product of their standard deviations. Mathematically, it's expressed as:</p> \\[\\boxed{ r = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (y_i - \\overline{y})^2}} = \\frac{S_{x y}}{\\sqrt{S_{x x} \\cdot S_{y y}}}} \\] <p>Here, \\(\\overline{x}\\) and \\(\\overline{y}\\) are the means of \\(x\\) and \\(y\\), respectively. This formula essentially scales the covariance between \\(x\\) and \\(y\\) by the product of their standard deviations, ensuring that \\(r\\) is dimensionless and ranges between -1 and +1. Also note</p>"},{"location":"08_Regression/Calculating%20metrics/#coefficient-of-determination-r2","title":"Coefficient of Determination (\\(r^2\\))","text":"<p>The coefficient of determination, known as \\(r^2\\), is the square of the Pearson correlation coefficient. It represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It is calculated simply by squaring \\(r\\):</p> \\[\\boxed{ r^2 = \\left(\\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (y_i - \\overline{y})^2}}\\right)^2} \\] <p>This value ranges from 0 to 1, where 0 indicates no correlation and 1 indicates perfect correlation.</p>"},{"location":"08_Regression/Calculating%20metrics/#alternate-formulas-for-r-and-r2-in-the-context-of-regression","title":"Alternate Formulas for \\(r\\) and \\(r^2\\) in the Context of Regression","text":"<p>In the context of simple linear regression, where you have calculated the slope (\\(\\beta_1\\)) and the intercept (\\(\\beta_0\\)), \\(r\\) and \\(r^2\\) can also be calculated directly from the regression output:</p>"},{"location":"08_Regression/Calculating%20metrics/#using-standard-deviations-and-slope","title":"Using Standard Deviations and Slope:","text":"<p>$$\\boxed{   r = \\beta_1 \\frac{s_x}{s_y}}   $$   where \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively, and \\(\\beta_1\\) is the slope of the regression line. This formula derives from the relationship that the slope of the regression line in standardized units is the correlation coefficient. Note, \\(s_x = \\sqrt{S_{xx}}\\) and Note, \\(s_y = \\sqrt{S_{yy}}\\), in both cases omitting \\(\\frac{1}{n-1}\\).</p>"},{"location":"08_Regression/Calculating%20metrics/#from-the-sum-of-squares","title":"From the Sum of Squares:","text":"<p>$$\\boxed{   r^2 = \\frac{SSR}{SST} = 1-\\frac{SSE}{SST}}   $$</p> <p>In regression analysis, the total sum of squares (SST), the regression sum of squares (SSR), and the sum of squares of errors (SSE) are important quantities for measuring the variability in the data and the performance of the regression model.</p> <ul> <li> <p>Total Sum of Squares (SST)</p> <p>The Total Sum of Squares measures the total variability of the dataset relative to the mean. It is calculated as: $$ \\text{SST} = \\sum_{i=1}^n (y_i - \\overline{y})^2 $$ where \\(y_i\\) are the observed values and \\(\\overline{y}\\) is the mean of the \\(y\\) values.</p> </li> <li> <p>Regression Sum of Squares (SSR)</p> <p>The Regression Sum of Squares measures how much of the total variability in the dependent variable can be explained by the independent variable(s) in the model. It is calculated as: $$ \\text{SSR} = \\sum_{i=1}^n (\\hat{y}_i - \\overline{y})^2 $$ where \\(\\hat{y}_i\\) are the predicted values from the regression model.</p> </li> <li> <p>Sum of Squares of Errors (SSE)</p> <p>The Sum of Squares of Errors measures the variability of the model errors (residuals). It is calculated as: $$ \\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$ where \\(y_i\\) are the observed values and \\(\\hat{y}_i\\) are the predicted values from the regression model.</p> </li> </ul> <p>These three components are related by the identity:    $$    \\text{SST} = \\text{SSR} + \\text{SSE}    $$    This identity shows that the total variability in the dataset (\\(\\text{SST}\\)) can be decomposed into the variability explained by the regression model (\\(\\text{SSR}\\)) and the variability that is not explained by the model (\\(\\text{SSE}\\)).</p> <p>Each of these formulas provides insight into different aspects of the regression analysis, such as the effectiveness of the model in explaining the variation in the data and the amount of error in the predictions.</p>"},{"location":"08_Regression/Calculating%20metrics/#from-the-z-scores","title":"From the \\(z\\)-scores:","text":"<p>The formula for calculating the Pearson correlation coefficient using standardized scores is:</p> \\[\\boxed{ r = \\frac{\\sum (z_x \\cdot z_y)}{n-1}} \\] <p>where: - \\(z_x = \\frac{x - \\overline{x}}{s_x}\\) and \\(z_y = \\frac{y - \\overline{y}}{s_y}\\) are the standardized scores of \\(x\\) and \\(y\\).  - \\(\\overline{x}\\) and \\(\\overline{y}\\) are the means of \\(x\\) and \\(y\\), respectively. - \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively. - The numerator, \\(\\sum (z_x \\cdot z_y)\\), represents the sum of the products of these standardized scores, effectively capturing the covariance of \\(x\\) and \\(y\\). - The denominator, \\(n-1\\), corrects for the bias in variance estimation from a sample, making the calculation an unbiased estimator of the population correlation coefficient.</p> <p>This approach normalizes both variables to have zero mean and unit variance, simplifying the interpretation of the correlation coefficient as it directly measures the degree of linear relationship between the standardized versions of the original variables.</p>"},{"location":"08_Regression/Calculating%20metrics/#breakdown-using-individual-sum-terms_1","title":"Breakdown Using Individual Sum Terms:","text":"<p>$$\\boxed{   r = \\frac{n \\cdot \\sum (x_i y_i) - \\sum x_i \\cdot \\sum y_i}{\\sqrt{n \\cdot \\sum x_i^2 - (\\sum x_i)^2} \\cdot \\sqrt{n \\cdot y_i^2 - (\\sum y_i)^2}}}   $$</p> <p>where:   - $ \\sum (x_i y_i) $ is the sum of the products of corresponding $ x $ and $ y $ values.   - $ \\sum x_i $ and $ \\sum y_i $ are the sums of all $ x $ values and $ y $ values, respectively.   - $ \\sum x_i^2 $ and $ \\sum y_i^2 $ are the sums of the squares of all $ x $ and $ y $ values, respectively.   - The term $ n $ is the number of data points.</p> <p>The denominator involves the square roots of the products of $ n $ and the sum of squares of $ x $ and $ y $, minus the square of the sum of $ x $ and $ y $ values, all scaled by $ n $. This structure follows the formula for the standard deviation, scaled to the sample size to adjust for bias, fitting the denominator of the correlation coefficient formula. This formula effectively measures the strength and direction of a linear relationship between two variables.</p>"},{"location":"08_Regression/Calculating%20metrics/#example-exam-2020-asignment-7","title":"Example: Exam 2020, Asignment 7:","text":"<p>Problem:</p> <p>A professor in the School of Engineering in a university polled a dozen colleagues about the number of professional meetings they attended in the past five years \\((x)\\) and the number of papers they submitted to refereed journals \\((y)\\) during the same period. The summary data are given as follows: $$ \\begin{aligned} n &amp; =12, \\quad \\bar{x}=4, \\quad \\bar{y}=12 \\ \\sum_{i=1}^n x_i^2 &amp; =232, \\quad \\sum_{i=1}^n x_i y_i=318 \\end{aligned} $$</p> <p>Fit a simple linear regression model between \\(x\\) and \\(y\\) by finding out the estimates of intercept and slope. Hint: Use the Least Squares Estimates formula from the book.</p> <p>Solution:</p> <p>Given the values, we can use one of the variations provided earlier:</p> \\[ \\beta_1 = \\frac{\\sum_{i=1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i=1}^n x_i^2 - n \\overline{x}^2} \\] <p>Let's plug in the values:</p> <ul> <li>\\(n = 12\\)</li> <li>\\(\\overline{x} = 4\\)</li> <li>\\(\\overline{y} = 12\\)</li> <li>\\(\\sum_{i=1}^n x_i^2 = 232\\)</li> <li>\\(\\sum_{i=1}^n x_i y_i = 318\\)</li> </ul> <p>Calculating \\(\\beta_1\\):</p> \\[ \\beta_1 = \\frac{318 - 12 \\times 4 \\times 12}{232 - 12 \\times 4^2} \\] <p>In the solution, the following formula, also presented above, is used:</p> \\[ \\beta_1=\\frac{n \\sum\\left(x_i y_i\\right)-\\sum x_i \\sum y_i}{n \\sum\\left(x_i^2\\right)-\\left(\\sum x_i\\right)^2} \\] <p>Plugging these values into the formula: $$ \\beta_1=\\frac{(12)(318)-[(12)(4)][(12)(12)]}{(12)(232)-[(12)(4)]^2} $$</p> <p>Once we have \\(\\beta_1\\), we can calculate the intercept \\(\\beta_0\\) using the formula:</p> \\[ \\beta_0 = \\overline{y} - \\beta_1 \\overline{x} \\] <p>We'll compute \\(\\beta_1\\) first and then use it to find \\(\\beta_0\\).</p> <p>Using the provided data, the estimated parameters for your linear regression model are:</p> <ul> <li>Slope (\\(\\beta_1\\)): \\(-6.45\\)</li> <li>Intercept (\\(\\beta_0\\)): \\(37.8\\)</li> </ul> <p>Thus, the fitted linear regression model can be expressed as: $$ y = 37.8 - 6.45x $$ This equation predicts the value of \\(y\\) based on the value of \\(x\\), with the model suggesting that \\(y\\) decreases by approximately 6.45 units for every one unit increase in \\(x\\).</p>"},{"location":"08_Regression/Test/","title":"Calculating Metrics in Simple Linear Regression","text":""},{"location":"08_Regression/Test/#the-slope-and-intercept","title":"The Slope (and intercept)","text":"<p>In simple linear regression, the slope parameter (often denoted as \\(\\beta_1\\)) represents the relationship between the independent variable (X) and the dependent variable (Y). The slope indicates how much the dependent variable is expected to increase (or decrease) for a one-unit increase in the independent variable. There are several ways to compute this slope, and here are some of the key methods:</p>"},{"location":"08_Regression/Test/#least-squares-estimation","title":"Least Squares Estimation:","text":"<p>The most common method for calculating the slope in simple linear regression is the least squares estimation. The formula for the slope (\\(\\beta_1\\)) using this method is:</p> \\[\\beta_1=\\frac{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)\\left(y_i-\\bar{y}\\right)}{\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2}\\] <p>In this expression:</p> <p>\\(\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\) is the sum of the products of the deviations of \\(x\\) and \\(y\\) from their respective means. This term captures the covariance between \\(x\\) and \\(y\\), indicating how much \\(x\\) and \\(y\\) vary together from their mean values (though excluding \\(\\frac{1}{n-1}\\)). This term is often denoted \\(S_{xy}\\).</p> <p>\\(\\overline{x}\\) and \\(\\overline{y}\\) are the means of \\(x\\) and \\(y\\), respectively. These are calculated as \\(\\overline{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\) and </p> <p>\\(\\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_i\\), where \\(n\\) is the number of observations.</p> <p>\\(\\sum_{i=1}^n (x_i - \\overline{x})^2\\) represents the sum of the squares of the deviations of \\(x\\) from its mean. This term is a measure of the total variance in \\(x\\) and helps normalize the covariance in the numerator. This term is often denoted \\(S_{xy}\\).</p> <p>\\(n\\) is the number of observations, indicating the total number of data points in the dataset.</p>"},{"location":"08_Regression/Test/#sum-of-products","title":"Sum of Products","text":"<p>The fundamental expression for the slope is the one stated above. But we can also use the sum of products which is demonstrated here.</p> <p>Derivation of \\((S_{xy})\\)</p> <p>The regular form for the sum of products of deviations for \\(x\\) and \\(y\\) is: \\(\\(S_{xy} = \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\)\\)</p> <p>Expanding this sum: \\(\\(S_{xy} = \\sum_{i=1}^n (x_i y_i - x_i \\overline{y} - y_i \\overline{x} + \\overline{x} \\overline{y})\\)\\)</p> <p>Simplifying this by distributing the summation: \\(\\(S_{xy} = \\sum_{i=1}^n x_i y_i - \\sum_{i=1}^n x_i \\overline{y} - \\sum_{i=1}^n y_i \\overline{x} + n \\overline{x} \\overline{y}\\)\\)</p> <p>Notice that $\\sum_{i=1}^n x_i \\overline{y} $ can be rewritten because \\(\\overline{y}\\) is a constant: \\(\\(\\sum_{i=1}^n x_i \\overline{y} = \\overline{y} \\sum_{i=1}^n x_i\\)\\) And similarly for \\(\\sum_{i=1}^n y_i \\overline{x}\\): \\(\\(\\sum_{i=1}^n y_i \\overline{x} = \\overline{x} \\sum_{i=1}^n y_i\\)\\)</p> <p>Since \\(\\sum_{i=1}^n x_i = n \\overline{x}\\) and \\(\\sum_{i=1}^n y_i = n \\overline{y}\\), the terms simplify to: \\(\\(S_{xy} = \\sum_{i=1}^n x_i y_i - n \\overline{x} \\overline{y}\\)\\)</p> <p>Derivation of \\(S_{xx}\\)</p> <p>The regular form for the sum of squares of deviations for $ x $ is: \\(\\(S_{xx} = \\sum_{i=1}^n (x_i - \\overline{x})^2\\)\\)</p> <p>Expanding this sum: \\(\\(S_{xx} = \\sum_{i=1}^n (x_i^2 - 2x_i \\overline{x} + \\overline{x}^2)\\)\\)</p> <p>Simplifying this by distributing the summation: \\(\\(S_{xx} = \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + n\\overline{x}^2\\)\\)</p> <p>Since \\(\\sum_{i=1}^n x_i = n\\overline{x}\\): \\(\\(S_{xx} = \\sum_{i=1}^n x_i^2 - 2n\\overline{x}^2 + n\\overline{x}^2\\)\\)</p> <p>Combining the terms results in: \\(\\(S_{xx} = \\sum_{i=1}^n x_i^2 - n\\overline{x}^2\\)\\)</p> <p>These derivations provide a clear mathematical pathway from the traditional definitions of $ S_{xy} $ and $ S_{xx} $ to the forms that are easily implemented in a python script for instance, and also means we can also formulate the slope as:</p> \\[\\boxed{\\beta_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i=1}^n x_i^2 - n\\overline{x}^2}}\\]"},{"location":"08_Regression/Test/#breakdown-using-individual-sum-terms","title":"Breakdown Using Individual Sum Terms","text":"<p>Another way to express this is by explicitly calculating the sums:</p> \\[\\boxed{\\beta_1=\\frac{n \\sum\\left(x_i y_i\\right)-\\sum x_i \\sum y_i}{n \\sum\\left(x_i^2\\right)-\\left(\\sum x_i\\right)^2}}\\] <p>In this expression: - \\(n\\) is the number of observations., - \\(\\sum\\left(x_i y_i\\right)\\) is the sum of the products of corresponding \\(x\\) and \\(y\\) values, - \\(\\sum x_i\\) and \\(\\sum y_i\\) are the sums of \\(x\\) and \\(y\\) values, respectively, - \\(\\sum\\left(x_i^2\\right)\\) is the sum of the squares of \\(x\\) values.</p>"},{"location":"08_Regression/Test/#covariance-and-variance-method","title":"Covariance and Variance Method:","text":"<p>This is essentially a rearrangement of the least squares formula, emphasizing the use of covariance and variance:    \\(\\(\\boxed{\\beta_1 = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)}}\\)\\)    where: - \\(\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{n-1}\\) (assuming a sample covariance) - \\(\\text{Var}(X) = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n-1}\\) (also assuming sample variance)</p>"},{"location":"08_Regression/Test/#matrix-algebra-using-normal-equation","title":"Matrix Algebra (Using Normal Equation):","text":"<p>When dealing with linear regression in matrix terms, the slope can be calculated using the normal equation:    $$ \\boxed{\\beta = (X^T X)^{-1} X^T Y}$$    Here, \\(X\\) is the matrix of input features (including a column of ones for the intercept if it's included in the model), and \\(Y\\) is the vector of output values, and \\(\\beta\\) is a vector that will contain all the coefficients.</p>"},{"location":"08_Regression/Test/#gradient-descent","title":"Gradient Descent:","text":"<p>Though not a formula in the traditional sense, gradient descent is an algorithmic approach used to find the minimum of the cost function (typically mean squared error) in regression. The update rule in each iteration for \\(\\beta_1\\) would be:    \\(\\(\\boxed{\\beta_1^{(new)} = \\beta_1^{(old)} - \\alpha \\frac{\\partial}{\\partial \\beta_1} MSE}\\)\\)    where \\(\\alpha\\) is the learning rate and \\(\\frac{\\partial}{\\partial \\beta_1} MSE\\) is the derivative of the mean squared error with respect to \\(\\beta_1\\).</p>"},{"location":"08_Regression/Test/#regression-line-intercept-inclusion","title":"Regression Line Intercept Inclusion","text":"<p>We will also explain how the slope relates to the intercept in the regression equation, presented here as part of the full regression formula:</p> \\[y = \\beta_0 + \\beta_1 x\\] <p>where \\(\\(\\beta_1 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n (x_i - \\overline{x})^2}\\)\\) (or one of the alternatives from above) and \\(\\(\\boxed{\\beta_0 = \\overline{y} - \\beta_1 \\overline{x}}\\)\\)</p>"},{"location":"08_Regression/Test/#correlation-coefficient-r-and-correlation-of-determination-r2","title":"Correlation Coefficient (\\(r\\)) and Correlation of Determination (\\(r^2\\))","text":"<p>This section will present different formulations of \\(r\\) or \\(r^2\\). Depending on the formulation, one or the other is shown.</p>"},{"location":"08_Regression/Test/#pearson-correlation-coefficient-r","title":"Pearson Correlation Coefficient (\\(r\\))","text":"<p>The Pearson correlation coefficient (\\(r\\)) measures the linear correlation between two variables, \\(x\\) and \\(y\\). It is defined as the ratio of the covariance of the variables to the product of their standard deviations. Mathematically, it's expressed as:</p> \\[\\boxed{r = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (y_i - \\overline{y})^2}} = \\frac{S_{x y}}{\\sqrt{S_{x x} \\cdot S_{y y}}}}\\] <p>Here, \\(\\overline{x}\\) and \\(\\overline{y}\\) are the means of \\(x\\) and \\(y\\), respectively. This formula essentially scales the covariance between \\(x\\) and \\(y\\) by the product of their standard deviations, ensuring that \\(r\\) is dimensionless and ranges between -1 and +1. Also note</p>"},{"location":"08_Regression/Test/#coefficient-of-determination-r2","title":"Coefficient of Determination (\\(r^2\\))","text":"<p>The coefficient of determination, known as \\(r^2\\), is the square of the Pearson correlation coefficient. It represents the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It is calculated simply by squaring \\(r\\):</p> \\[\\boxed{r^2 = \\left(\\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\overline{x})^2 \\sum_{i=1}^n (y_i - \\overline{y})^2}}\\right)^2}\\] <p>This value ranges from 0 to 1, where 0 indicates no correlation and 1 indicates perfect correlation.</p>"},{"location":"08_Regression/Test/#alternate-formulas-for-r-and-r2-in-the-context-of-regression","title":"Alternate Formulas for \\(r\\) and \\(r^2\\) in the Context of Regression","text":"<p>In the context of simple linear regression, where you have calculated the slope (\\(\\beta_1\\)) and the intercept (\\(\\beta_0\\)), \\(r\\) and \\(r^2\\) can also be calculated directly from the regression output:</p>"},{"location":"08_Regression/Test/#using-standard-deviations-and-slope","title":"Using Standard Deviations and Slope:","text":"<p>\\(\\(\\boxed{r = \\beta_1 \\frac{s_x}{s_y}}\\)\\)   where \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively, and \\(\\beta_1\\) is the slope of the regression line. This formula derives from the relationship that the slope of the regression line in standardized units is the correlation coefficient. Note, \\(s_x = \\sqrt{S_{xx}}\\) and Note, \\(s_y = \\sqrt{S_{yy}}\\), in both cases omitting \\(\\frac{1}{n-1}\\).</p>"},{"location":"08_Regression/Test/#from-the-sum-of-squares","title":"From the Sum of Squares:","text":"<p>\\(\\(\\boxed{r^2 = \\frac{SSR}{SST} = 1-\\frac{SSE}{SST}}\\)\\)</p> <p>In regression analysis, the total sum of squares (SST), the regression sum of squares (SSR), and the sum of squares of errors (SSE) are important quantities for measuring the variability in the data and the performance of the regression model.</p> <ul> <li> <p>Total Sum of Squares (SST)</p> <p>The Total Sum of Squares measures the total variability of the dataset relative to the mean. It is calculated as: \\(\\(\\text{SST} = \\sum_{i=1}^n (y_i - \\overline{y})^2\\)\\) where \\(y_i\\) are the observed values and \\(\\overline{y}\\) is the mean of the \\(y\\) values.</p> </li> <li> <p>Regression Sum of Squares (SSR)</p> <p>The Regression Sum of Squares measures how much of the total variability in the dependent variable can be explained by the independent variable(s) in the model. It is calculated as: \\(\\(\\text{SSR} = \\sum_{i=1}^n (\\hat{y}_i - \\overline{y})^2\\)\\) where \\(\\hat{y}_i\\) are the predicted values from the regression model.</p> </li> <li> <p>Sum of Squares of Errors (SSE)</p> <p>The Sum of Squares of Errors measures the variability of the model errors (residuals). It is calculated as: \\(\\(\\text{SSE} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\)\\) where \\(y_i\\) are the observed values and \\(\\hat{y}_i\\) are the predicted values from the regression model.</p> </li> </ul> <p>These three components are related by the identity:    \\(\\(\\text{SST} = \\text{SSR} + \\text{SSE}\\)\\)    This identity shows that the total variability in the dataset (\\(\\text{SST}\\)) can be decomposed into the variability explained by the regression model (\\(\\text{SSR}\\)) and the variability that is not explained by the model (\\(\\text{SSE}\\)).</p> <p>Each of these formulas provides insight into different aspects of the regression analysis, such as the effectiveness of the model in explaining the variation in the data and the amount of error in the predictions.</p>"},{"location":"08_Regression/Test/#from-the-z-scores","title":"From the \\(z\\)-scores:","text":"<p>The formula for calculating the Pearson correlation coefficient using standardized scores is:</p> \\[\\boxed{r = \\frac{\\sum (z_x \\cdot z_y)}{n-1}}\\] <p>where: - \\(z_x = \\frac{x - \\overline{x}}{s_x}\\) and \\(z_y = \\frac{y - \\overline{y}}{s_y}\\) are the standardized scores of \\(x\\) and \\(y\\).  - \\(\\overline{x}\\) and \\(\\overline{y}\\) are the means of \\(x\\) and \\(y\\), respectively. - \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(x\\) and \\(y\\), respectively. - The numerator, \\(\\sum (z_x \\cdot z_y)\\), represents the sum of the products of these standardized scores, effectively capturing the covariance of \\(x\\) and \\(y\\). - The denominator, \\(n-1\\), corrects for the bias in variance estimation from a sample, making the calculation an unbiased estimator of the population correlation coefficient.</p> <p>This approach normalizes both variables to have zero mean and unit variance, simplifying the interpretation of the correlation coefficient as it directly measures the degree of linear relationship between the standardized versions of the original variables.</p>"},{"location":"08_Regression/Test/#breakdown-using-individual-sum-terms_1","title":"Breakdown Using Individual Sum Terms:","text":"<p>\\(\\(\\boxed{r = \\frac{n \\cdot \\sum (x_i y_i) - \\sum x_i \\cdot \\sum y_i}{\\sqrt{n \\cdot \\sum x_i^2 - (\\sum x_i)^2} \\cdot \\sqrt{n \\cdot y_i^2 - (\\sum y_i)^2}}}\\)\\)</p> <p>where:   - $ \\sum (x_i y_i) $ is the sum of the products of corresponding $ x $ and $ y $ values.   - $ \\sum x_i $ and $ \\sum y_i $ are the sums of all $ x $ values and $ y $ values, respectively.   - $ \\sum x_i^2 $ and $ \\sum y_i^2 $ are the sums of the squares of all $ x $ and $ y $ values, respectively.   - The term $ n $ is the number of data points.</p> <p>The denominator involves the square roots of the products of $ n $ and the sum of squares of $ x $ and $ y $, minus the square of the sum of $ x $ and $ y $ values, all scaled by $ n $. This structure follows the formula for the standard deviation, scaled to the sample size to adjust for bias, fitting the denominator of the correlation coefficient formula. This formula effectively measures the strength and direction of a linear relationship between two variables.</p>"},{"location":"08_Regression/Test/#example-exam-2020-asignment-7","title":"Example: Exam 2020, Asignment 7:","text":"<p>Problem:</p> <p>A professor in the School of Engineering in a university polled a dozen colleagues about the number of professional meetings they attended in the past five years \\((x)\\) and the number of papers they submitted to refereed journals \\((y)\\) during the same period. The summary data are given as follows: \\(\\(\\begin{aligned} n &amp; =12, \\quad \\bar{x}=4, \\quad \\bar{y}=12 \\\\ \\sum_{i=1}^n x_i^2 &amp; =232, \\quad \\sum_{i=1}^n x_i y_i=318 \\end{aligned}\\)\\)</p> <p>Fit a simple linear regression model between \\(x\\) and \\(y\\) by finding out the estimates of intercept and slope. Hint: Use the Least Squares Estimates formula from the book.</p> Click here to see the solution <p>Given the values, we can use one of the variations provided earlier:  $$\\beta_1 = \\frac{\\sum_{i=1}^n x_i y_i - n \\overline{x} \\overline{y}}{\\sum_{i=1}^n x_i^2 - n \\overline{x}^2}$$  Let's plug in the values:  - $n = 12$ - $\\overline{x} = 4$ - $\\overline{y} = 12$ - $\\sum_{i=1}^n x_i^2 = 232$ - $\\sum_{i=1}^n x_i y_i = 318$  Calculating $\\beta_1$:  $$\\beta_1 = \\frac{318 - 12 \\times 4 \\times 12}{232 - 12 \\times 4^2}$$  In the solution, the following formula, also presented above, is used:  $$\\beta_1=\\frac{n \\sum\\left(x_i y_i\\right)-\\sum x_i \\sum y_i}{n \\sum\\left(x_i^2\\right)-\\left(\\sum x_i\\right)^2}$$  Plugging these values into the formula: $$\\beta_1=\\frac{(12)(318)-[(12)(4)][(12)(12)]}{(12)(232)-[(12)(4)]^2}$$   Once we have $\\beta_1$, we can calculate the intercept $\\beta_0$ using the formula:  $$\\beta_0 = \\overline{y} - \\beta_1 \\overline{x}$$  We'll compute $\\beta_1$ first and then use it to find $\\beta_0$.  Using the provided data, the estimated parameters for your linear regression model are:  - Slope ($\\beta_1$): $-6.45$ - Intercept ($\\beta_0$): $37.8$  Thus, the fitted linear regression model can be expressed as: $$y = 37.8 - 6.45x$$ This equation predicts the value of $y$ based on the value of $x$, with the model suggesting that $y$ decreases by approximately 6.45 units for every one unit increase in $x$.</p>"},{"location":"09_Introduction_to_Stochastic_Processes/","title":"09 Introduction to Stochastic Processes","text":"09 Introduction to Stochastic Processes"},{"location":"09_Introduction_to_Stochastic_Processes/#material","title":"Material:","text":"<p>Markov Chains Ch. 1 (the rest is not in the syllabus)</p> <p>Matrix Algebra (optional)</p> <p>Recap Regression</p> <p>Session notes</p> <p>Session material</p> <p>Session from 20/21: SMP 9</p>"},{"location":"09_Introduction_to_Stochastic_Processes/#topics","title":"Topics","text":"<p>The topics of this and next week require a bit of knowledge of matrices and matrix algebra. I recommend reading \u201cMatrix Algebra\u201d which is the chapter we use from the Linear Algebra course. If you already passed the Linear Algebra course (i.e. you are on your 7<sup>th</sup> semester), you will probably not need to worry about this prerequisite.</p> <p>Stochastic processes are mathematical models used to describe the evolution of systems that involve randomness. The most important elements of stochastic processes are their underlying probability distributions and the properties of their randomness, such as stationarity, independence, and Markovianity. The probability distribution of a stochastic process describes the likelihood of different possible outcomes at any given time, while the properties of randomness determine how the outcomes are related to each other over time. Stochastic processes can be classified into discrete-time or continuous-time, and they can be used to model a wide range of phenomena, including financial markets, stock prices, traffic flow, weather patterns, and biological systems.</p> <ul> <li>What is a random/stochastic process (as opposed to a random variable)?</li> <li>Poisson process</li> <li>Random walk</li> <li>Markov chains</li> </ul> <p>The first two items are not directly covered in the literature and will be based on what I go through during class.</p>"},{"location":"09_Introduction_to_Stochastic_Processes/#problems-to-be-worked-on-inafter-class","title":"Problems to be worked on in/after class:","text":"<p>None specifically, but do some Wiseflow exercises</p>"},{"location":"10_Markov_Chains/","title":"10 Markov Chains","text":"10 Markov Chains <p>Material:</p> <p>Recap notes</p> <p>Session notes</p> <p>Session material</p> <p>Session from 20/21: SMP 10</p>"},{"location":"10_Markov_Chains/#topics","title":"Topics","text":"<p>Markov chains are a mathematical framework used to model systems that change over time, such as the weather or the stock market. The key feature of a Markov chain is that it assumes that the future state of the system depends only on its current state, and not on any previous states. This is known as the Markov property. Markov chains are defined by a set of states, a transition matrix that describes the probabilities of moving from one state to another, and an initial state. The long-term behavior of a Markov chain can be analyzed using techniques such as finding the stationary distribution or calculating expected values. Markov chains are used in a wide range of applications, including computer science, physics, finance, and biology, among others.</p> <ul> <li>Markov property: The assumption that the future state of a system depends only on its current state, and not on any previous states.</li> <li>State: A possible condition or configuration of the system being modeled.</li> <li>Transition matrix: A matrix that describes the probabilities of moving from one state to another.</li> <li>Stationary distribution: The long-term distribution of states that a Markov chain approaches over time.</li> <li>Expected value: The average value that a variable takes over many iterations of the Markov chain.</li> </ul>"},{"location":"10_Markov_Chains/#problems-to-be-worked-on-in-class","title":"Problems to be worked on in class:","text":"<p>Do Problems 10 and the Wiseflow exam cases covering the final topics.</p>"},{"location":"11_Recap_and_Exercises_Markov_Chains/","title":"11 Recap and Exercises","text":"11 Recap and Exercises Markov Chains <p>Material:</p> <p>Session notes</p> <p>Session exercises - Problems 10</p> <p>Session material</p>"},{"location":"11_Recap_and_Exercises_Markov_Chains/#topics","title":"Topics","text":"<p>Recap and exercises in last weeks topic.</p>"},{"location":"11_Recap_and_Exercises_Markov_Chains/#problems-to-be-worked-on-in-class","title":"Problems to be worked on in class:","text":"<p>Do Problems the Wiseflow exam cases covering the final topics as well as previous exams in Wiseflow.</p>"},{"location":"Sessions/","title":"Sessions","text":"<p>Click on a session to the left to access a plan of a specific session and additional resources for that session. The plan includes material for both the lectures and recitations.</p> Session Date Topic 00 Important Recap 01 9 Feb 08:20 \u2013 11:50 Introduction + Recap Probability + Stochastic Variables 02 16 Feb 08:20 \u2013 11:50 Discrete Random Variables 03 23 Feb 08:20 \u2013 11:50 Continuous Random Variables 04 1 Mar 08:20 \u2013 11:50 Multivariate Random Variables 05 8 Mar 08:20 \u2013 11:50 Point Estimation and sampling 06 15 Mar 08:20 \u2013 11:50 Statistical Intervals 07 22 Mar 08:20 \u2013 11:50 Hypothesis Testing 08 12 Apr 08:20 \u2013 11:50 Regression 09 19 Apr 08:20 \u2013 11:50 Introduction to Stochastic Processes 10 26 Apr  08:20 \u2013 11:50 Markov Chains 11 3 May  08:20 \u2013 11:50 Recap and Exercises Markov Chains"},{"location":"blog/","title":"Blog","text":""},{"location":"pages/exam/","title":"Eksamen","text":"<p>Eksamen best\u00e5r af tre dele, der samlet skal bed\u00f8mmes til best\u00e5et for at best\u00e5 eksamen. De tre dele er:</p> <ol> <li> <p>En skriftlig hjemmeopgave i form af en rapport. Rapporten m\u00e5 maksimalt have et omfang p\u00e5 5 sider \u00e1 2.400 anslag inklusive mellemrum, men eksklusiv indholdsfortegnelse, litteraturliste og eventuelle bilag. Rapporten m\u00e5 skrives individuelt eller i grupper af maksimalt 3 studerende. For hvert ekstra medlem i en gruppe, m\u00e5 der skrives 1 side \u00e1 2.400 anslag yderligere. Opgaven t\u00e6ller 1 ECTS. S\u00e5fremt hverken rapporten eller den samlede eksamen er best\u00e5et, skal rapporten genafleveres til reeksamen.</p> </li> <li> <p>En skriftlig 3 timers individuel stedpr\u00f8ve i form af en bunden opgave. Opgaven t\u00e6ller 2 ECTS. S\u00e5fremt hverken opgaven eller den samlede eksamen er best\u00e5et, skal opgaven genafleveres til reeksamen.</p> </li> <li> <p>En skriftlig 1 times individuel MCQ-pr\u00f8ve. Pr\u00f8ven t\u00e6ller 1 ECTS. S\u00e5fremt hverken pr\u00f8ven eller den samlede eksamen er best\u00e5et, skal pr\u00f8ven genafleveres til reeksamen.</p> </li> </ol>"},{"location":"pages/faq/","title":"FAQ","text":""},{"location":"pages/faq/#generelt-om-kurset","title":"Generelt om kurset","text":"Hvad er kursets m\u00e5l og l\u00e6ringsudbytte? <p>Kursets m\u00e5l er at give de studerende en grundl\u00e6ggende forst\u00e5else af...</p> <p>L\u00e6ringsudbytte inkluderer: - Forst\u00e5else af centrale begreber. - Anvendelse af teorier i praksis.</p> Hvordan relaterer kurset sig til min uddannelse? <p>Kurset er en integreret del af uddannelsens fokus p\u00e5...</p>"},{"location":"pages/faq/#eksamen-og-vurdering","title":"Eksamen og vurdering","text":"Hvordan bliver jeg vurderet? <p>Du bliver vurderet gennem: - En skriftlig eksamen (50%). - Et gruppeprojekt (50%).</p> Hvorn\u00e5r og hvordan afholdes eksamen? <p>Eksamen afholdes i slutningen af semesteret. Detaljer findes i kursusplanen.</p>"},{"location":"pages/faq/#teknisk-support","title":"Teknisk support","text":"Hvad g\u00f8r jeg, hvis jeg oplever tekniske problemer? <p>Hvis du oplever tekniske problemer, kan du: - Kontakte support via helpdesk@example.com. - Tjekke vejledningen i kursusmaterialet.</p>"}]}