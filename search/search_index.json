{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"MAL1 <p>Jump to Lesson Plan</p> <p>Jump to Portfolios and Project</p> <p>This repository contains resources for the Machine Learning &amp; AI course at VIA University College, Horsens, Denmark.</p> <p>The course consists of 12 scheduled sessions, each with a duration of 2-4 lessons. The sessions start in week 6 and the final session will be in week 19. During these weeks, the students work on a self-chosen group project. Additionally, there are 6 group assignments during the semester which have specified deadlines. Both the project and the assignments must be handed in on Itslearning.</p> <p>The sessions will be taught by Frederik Thorning Bj\u00f8rn (FRBJ) and Richard Brooks (RIB).</p>"},{"location":"#literature-resources","title":"Literature &amp; Resources","text":"<p>G\u00e9ron, Aur\u00e9lien: Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow, 3<sup>rd</sup> Edition. (the second edition will also do)</p> <p>We highly recommend retrieving a copy of the book \u2013 it will also be the course book for the Deep Learning course in the Autumn.</p>"},{"location":"#software","title":"Software","text":"<p>Make sure you install a working version of Jupyter Notebook and Python version 3.7 or higher. The easiest way to install Python and Jupyter is using Anaconda Distribution. You can choose whichever framework you want to work in as long as it can handle Jupyter Notebooks. Installing VS Code with a Jupyter Notebook extension seems to be a popular choice.</p> <p>The course will be somewhat \"Python-heavy\" and during the course, it is expected that you can solve relatively complex machine learning problems in Python (in your assignments and project). It is expected that you are able to work in Python or learn to do so relatively fast.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>There are three main areas that are important when it comes to Machine Learning</p> <ul> <li>Programming</li> <li>Linear Algebra</li> <li>Probability theory and statistics.</li> </ul> <p>We assume that you have (1) covered! Linear Algebra knowledge you can obtain either by following our course (IT-ALI1) or you can see some suggestions below under Online Resource. The same goes for item (3). Now, it is possible to master machine learning without knowing anything about linear algebra or probability theory, but some topics will most likely be easier to comprehend if you have some background knowledge about the underlying mathematical foundation.</p>"},{"location":"#online-resources","title":"Online Resources","text":"<p>There are several Python-programming tutorials on YouTube, also ones that are data science / Machine Learning oriented. I recommend Alexander Ihler\u2019s course Machine Learning and Data Mining.</p> <p>For prerequisites, we have our own course here at VIA called Applied Linear Algebra. You can find an online version of the course at the course web page that also contains recordings from all sessions (from 2023).</p> <p>In terms of probability theory and statistics, we have our own course here at VIA called Stochastic Modelling and Processing (IT-SMP1). You can find an online version of the course at the course web page that also contains recordings from all sessions (from 2021).</p>"},{"location":"#historical-notes","title":"Historical Notes","text":"<p>Introduction to Machine Learning was first offered in the spring of 2018 and has been scheduled 1-2 times per year since then. The course responsible is Richard Brooks (RIB).</p>   **Grade Distribution 2023 (ordinary exam only)**  | Grade | Count | |-------|-------| | 12    | 13    | | 10    | 7     | | 7     | 15    | | 4     | 7     | | 02    | 5     | | 00    | 6     |"},{"location":"#lesson-plan","title":"Lesson Plan","text":"<p>Click on a session below to access a plan of a specific session and additional resources for that session.</p> Session Week Teacher Topic 01 6 RIB Introduction: Machine learning fundamentals 02 7 RIB Na\u00efve Bayes and support vector machines 03 8 FRBJ Tree-based models 04 9 RIB The machine learning pipeline: Introduction to final project 05 10 RIB Data preparation and feature engineering 06 11 FRBJ Validation methods and performance metrics 07 12 RIB The role of Linear Algebra in Machine Learning and AI 08 15 FRBJ Regression 09 16 RIB Dimensionality reduction 10 17 FRBJ Clustering 11 18 FRBJ Introduction to neural networks 12 19 FRBJ Perspectives in artificial intelligence"},{"location":"#portfolios-and-project","title":"Portfolios and Project","text":"<p>Click on the assignment to view the assignment. You must hand in to Itslearning (requires log in).</p> Assignment Deadline 1. Getting started: Airbnb 8.20 on Feb. 19 2. Classification: The Candidates Part 1 8.20 on Mar. 4 3. Feature Engineering and Preprocessing: Waterworks 8.20 on Mar. 18 4. Regression: Long-Term Correction of Wind Data 23.59 on Apr. 15 5. Dimensionality Reduction and Clustering: The Candidates Part 2 23.59 on Apr. 29 6. Neural Networks: Sentiment Analysis 23.59 on May 17 7. Final Group Project 23.59 on May 24"},{"location":"01_Introduction_-_Machine%20learning%20fundamentals/","title":"Index","text":"01 Introduction to Linear Algebra"},{"location":"01_Introduction_-_Machine%20learning%20fundamentals/#material","title":"Material:","text":"<p>Ch 1 + \"kNN\" (see Session material below) +  (Section about \"Logistic Regression\" in Ch 4, pp. 136 - 146)</p> <p>Session material</p> <p>Classifier comparison</p>"},{"location":"01_Introduction_-_Machine%20learning%20fundamentals/#topics","title":"Topics","text":"<p>This lecture will cover basic machine learning methodology, the logistic regression algorithm, and the k Nearest Neighbors algorithm.</p> <p>After attending this lecture and reading the corresponding part of the book, I expect you to be able to:</p> <ul> <li>Explain what is meant by the term Machine Learning (ML)</li> <li>Explain what is meant by supervised vs. unsupervised learning</li> <li>Explain the overall difference between classification and regression</li> <li>Describe the \"train-test\" methodology</li> <li>Train a logistic regression (LR) and k Nearest Neighbors (kNN) algorithm on a dataset in sklearn</li> <li>Explain the principles behind the kNN algorithm</li> <li>Explain the key ideas behind logistic regression, and implement a logistic regression classifier in python.</li> <li>Explain the concept of \u201cmaximum likelihood\u201d and how it is used.</li> <li>Explain and use L1 and L2 regularization in the context of logistic regression, and discuss the difference between these approaches, as well as the importance of the hyperparameter C.</li> <li>Discuss advantages and disadvantages of logistic regression.</li> <li>Explain what is meant by the \"hyperparameters\" of an algorithm</li> </ul>"},{"location":"02%20Naive%20Bayes%20and%20support%20vector%20machines/","title":"Index","text":"02 Classification - Na\u00efve Bayes &amp; Support vector machine"},{"location":"02%20Naive%20Bayes%20and%20support%20vector%20machines/#material","title":"Material:","text":"<p>Ch 5 (Up to and including \"Nonlinear SVM Classification\", pp. 147 - 157) + Na\u00efve Bayes</p> <p>Session material</p> <p>Session notes (Monday)</p> <p>Session notes (Tuesday)</p>"},{"location":"02%20Naive%20Bayes%20and%20support%20vector%20machines/#topics","title":"Topics","text":"<p>This lecture will delve into more sophisticated classification methods within machine learning. We will explore the theory and application of Support Vector Machines (SVM) and Naive Bayes classifiers.</p> <p>After attending this lecture and reading the corresponding part of the book, I expect you to be able to:</p> <ul> <li>Explain support vectors for linearly separable data, and how support vectors influence the decision boundary.</li> <li>Explain and exemplify how adding new features can make non-linearly separable data linearly separable.</li> <li>Discuss the key ideas behind the kernel trick and how this is used in kernelized support vector machines.</li> <li>Discuss how, when using a Gaussian kernel,\u00a0the hyperparameters C and\u00a0\u03b3 influence the decision boundaries.</li> <li>Discuss advantages and disadvantages of support vector machines.</li> <li>Understand the probabilistic foundations of the Naive Bayes classifier and its assumptions about feature independence.</li> <li>Describe the different types of Naive Bayes classifiers (e.g., Gaussian, Multinomial) and their appropriate application contexts.</li> <li>Apply a Naive Bayes classifier to text data and other types of datasets using sklearn.</li> <li>Discuss the concept of hyperparameters in the context of SVM and Naive Bayes, and demonstrate how to tune them to improve model performance.</li> </ul>"},{"location":"03%20Tree-based%20models/","title":"Index","text":"03 Tree-based Models"},{"location":"03%20Tree-based%20models/#material","title":"Material:","text":"<p>Ch 6 + 7</p> <p>Session material: In this folder.</p> <p>You will need to install the graphviz and pydotplus modules in python. You can do this by following these steps:</p> <p>1) Open the anaconda prompt 2) Install graphviz by typing <code>conda install python-graphviz</code> 3) Install pydotplus by typing <code>conda install pydotplus</code></p> <p>Alternatively, if the above doesn't work:</p> <p>1) Go to https://graphviz.gitlab.io/_pages/Download/Download_windows.html    and download graphviz  </p> <p>2) Add the following to your code:</p> <pre><code>import os  \nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n</code></pre>"},{"location":"03%20Tree-based%20models/#topics","title":"Topics","text":"<p>This lecture covers decision trees and related methods. We will talk about:</p> <ul> <li>Decision trees</li> <li>Random forests</li> <li>Gradient-boosted decision trees</li> </ul> <p>After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:</p> <ul> <li>Use and implement decision trees, random forests and gradient boosted decision trees in python.  </li> <li>Describe the advantages and disadvantages of using decision trees, random forests and gradient boosted decision trees, respectively.</li> <li>Visualize decision trees in different ways.</li> <li>Extract and interpret feature importance.</li> <li>Describe how the Gini impurity index can be used to determine which feature to branch off on.</li> <li>Explain what is meant by pre-pruning.</li> <li>Explain how random forests are random, including what is meant by bootstrapping and feature selection in this context.</li> <li>Explain what is meant by soft voting.</li> <li>Discuss different hyperparameters of tree-based methods, and how tuning these parameters influence the results.</li> </ul>"},{"location":"04%20The%20machine%20learning%20pipeline%20-%20Introduction%20to%20final%20project/","title":"Index","text":"04 The Machine Learning Pipeline"},{"location":"04%20The%20machine%20learning%20pipeline%20-%20Introduction%20to%20final%20project/#material","title":"Material:","text":"<p>Ch 2 (also for next week)</p> <p>Session material</p>"},{"location":"04%20The%20machine%20learning%20pipeline%20-%20Introduction%20to%20final%20project/#topics","title":"Topics","text":"<p>In this session we will go through how to complete a Machine Learning project from defining the problem to building the final model:</p> <ol> <li>Look at the big picture.</li> <li>Get the data.</li> <li>Discover and visualize the data to gain insights.</li> <li>Prepare the data for Machine Learning algorithms.</li> <li>Select a model and train it.</li> <li>Fine-tune your model.</li> </ol> <p>In addition, we will go through your final group project.</p> <p>After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:</p> <ul> <li>Define and articulate the scope of a Machine Learning problem, including understanding the big picture and setting clear objectives.</li> <li>Acquire and prepare datasets for Machine Learning projects, understanding the complexity of real-world data and the importance of pre-processing.</li> <li>Understand the relevance of exploratory data analysis to uncover insights and trends within the data, utilizing visualization techniques to aid in understanding.</li> <li>Understand why it is essential to perform model optimization and fine-tuning techniques to improve performance and achieve robustness in predictions, including understanding the importance of hyperparameter tuning.</li> </ul>"},{"location":"05%20Data%20preparation%20and%20feature%20engineering/","title":"Index","text":"05 Data Preparation and Feature Engineering"},{"location":"05%20Data%20preparation%20and%20feature%20engineering/#material","title":"Material:","text":"<p>Ch 2</p> <p>Session material</p>"},{"location":"05%20Data%20preparation%20and%20feature%20engineering/#topics","title":"Topics","text":"<p>This lecture will cover the basics of how to prepare a data set for machine learning and add new features based on those already in the data set. The topic includes:</p> <ul> <li>Data cleaning: What to do with missing values, outliers, nonsensical values</li> <li>Box Plots and other exploratory visualisations</li> <li>Data imputation</li> <li>Feature engineering:</li> <li>Dummy variables</li> <li>Scaling</li> </ul> <p>After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:</p> <ul> <li>Know how to approach features and when to drop and/or engineer them for your specific purposes</li> <li>Prepare a dataset for ML. Specifically, you should be able to explain and perform each of the operations below:</li> <li>Handle missing values/NaN-values in appropriate ways</li> <li>Identify and handle outliers using a boxplot</li> <li>Create dummy variables</li> <li>Scale/normalize variables</li> <li>Create a bag-of-words-representation for text-data</li> </ul>"},{"location":"06%20Validation%20methods%20and%20performance%20metrics/","title":"Index","text":"06 Validation Methods and Performance Metrics"},{"location":"06%20Validation%20methods%20and%20performance%20metrics/#material","title":"Material:","text":"<p>Ch 3</p> <p>Session material: In this folder</p>"},{"location":"06%20Validation%20methods%20and%20performance%20metrics/#topics","title":"Topics","text":"<p>This lecture will cover several machine learning methodologies:</p> <ul> <li>The train/test-methodelogy</li> <li>The validation set methodology</li> <li>The cross-validation methodology</li> <li>The leave-one-out-methodology</li> </ul> <p>In addition to this, we will discuss what is meant by true/false positives/negatives, and explore alternative performance metrics (so far, we have only encountered the \u201caccuracy\u201d-metric):</p> <ul> <li>Accuracy</li> <li>Confusion matrix</li> <li>Recall</li> <li>Precision</li> <li>F1-score</li> <li>Precision-recall-curve</li> </ul> <p>After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:</p> <ul> <li>Describe the \"validation set\"-methodology</li> <li>Describe the \"cross validation\"-methodology</li> <li>Describe the \"leave one out\"-methodology</li> <li>Apply each of the 3 methodologies above in sklearn</li> <li>Do hyperparameter tuning in sklearn using each of the 3 methodologies above (e.g. using the GridSearchCV-function in sklearn)</li> <li>Explain and calculate (in python) the following performance metrics for supervised classification:</li> <li>Confusion matrix</li> <li>Accuracy</li> <li>Recall ( = \"True Positive Rate\" (TPR) )</li> <li>Precision ( = \"Positive Prediction Rate\" (PPR) )</li> <li>F1-score</li> <li>Precision-recall-curve</li> </ul> <p>Useful resources:</p> <p>Performance Metrics for Classification Problems</p> <p>Performance Metrics in Machine Learning</p> <p>24 Evaluation Metrics for Binary Classification</p>"},{"location":"07%20The%20role%20of%20Linear%20Algebra%20in%20Machine%20Learning%20and%20AI/","title":"Index","text":"07 Linear Algebra in Machine Learning"},{"location":"07%20The%20role%20of%20Linear%20Algebra%20in%20Machine%20Learning%20and%20AI/#material","title":"Material:","text":"<p>Watch this video</p> <p>Session material</p> <p>Session notes X - Monday</p> <p>Session notes Y - Tuesday</p> <p>For a very appealing and visual explanation of SVD, you should take a look at Visual Kernel's video on the topic.</p> <p>Expect this session to last more than two lessons.</p>"},{"location":"07%20The%20role%20of%20Linear%20Algebra%20in%20Machine%20Learning%20and%20AI/#topics","title":"Topics","text":"<p>In today's session we look at some of the math behind Machine Learning. Ideally, you would have had both a course in Linear Algebra and Probability theory before starting up on Machine Learning, but in this session we will try to mediate this:</p> <ul> <li>Introduction to the importance of mathematics in machine learning.</li> <li>The role of linear algebra in representing data and models.</li> <li>The significance of GPUs and matrix operations in machine learning.</li> <li>Viewing linear algebra through the lens of programming concepts.</li> <li>Applications of linear algebra techniques like SVD in machine learning.</li> </ul> <p>After attending this lecture and/or watching the video, I expect you to be able to: - Understand why mathematics, particularly linear algebra, calculus, and statistics, is fundamental to machine learning and its optimization processes - Gain knowledge on how linear algebra facilitates the representation of machine learning data, models, and computations using arrays and matrices. - Recognize the critical role of GPUs and matrix operations in enhancing the efficiency and capabilities of machine learning algorithms. - Develop an intuitive grasp of linear algebra by relating it to programming concepts. - Learn about the application of linear algebra techniques, such as Singular Value Decomposition, in processing and optimizing machine learning data and models.</p>"},{"location":"08%20Regression/","title":"Index","text":"08 Regression"},{"location":"08%20Regression/#material","title":"Material:","text":"<p>Ch 4 (except \"Logistic Regression\")</p> <p>These are some useful resources for some of the main concepts:</p> <p>Linear regression, Regularization</p> <p>In general we recommend Alexander Ihler\u2019s course \u201cMachine Learning and Data Mining\u201d.</p> <p>Session material: In this folder.</p>"},{"location":"08%20Regression/#topics","title":"Topics","text":"<p>Today, we are going to look at regression algorithms, where instead of predicting a class you predict some continuous variable.</p> <p>We mainly focus on linear regression algorithms:</p> <ul> <li>Ordinary Least Squares (OLS) regression</li> <li>Ridge regression</li> <li>Lasso regression</li> </ul> <p>We also discuss (again) what is meant by \u201cregularization\u201d and consider the R\u00b2 performance metric for regression. Last but not least we will introduce one of the most important concepts in Machine Learning: Gradient Descent.</p> <p>After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:</p> <ul> <li>Explain what is meant by \"regression\" and in which contexts to apply it</li> <li>Explain the following linear regression models, their strengths and weaknesses, and apply them in python:</li> <li>Ordinary Least Squares (OLS) regression</li> <li>Ridge regression</li> <li>Lasso regression</li> <li>Elastic Net Regression</li> <li>Explain what is meant by the term \"regularization\" in an ML-context</li> <li>Describe what is meant by \"bias\" and \"variance\" in relation to ML-algorithms</li> <li>Explain the R\u00b2-metric for evaluating the performance of a linear regression algorithm</li> <li>Explain the MSE metric</li> <li>Explain how regression models can be trained</li> </ul>"},{"location":"09%20Introduction%20to%20neural%20networks/","title":"Index","text":"11 Introduction to Neural Networks"},{"location":"09%20Introduction%20to%20neural%20networks/#material","title":"Material:","text":"<p>Ch 10 + 11 until (but not including) \u201cFaster Optimizers\u201d (pp. 357 \u2013 378)</p> <p>For background and as reference: Ch 12</p> <p>3Blue1Brown: Neural networks (video series)</p> <p>Session material: In this folder</p> <p>You will need to install tensorflow and (optionally) tensorboard before this lesson. Specifically, you need tensorflow 2.0 or higher to be able to use all the functionalities in the notebooks. If you don't know whether you have these packages installed, you can open an anaconda prompt and type</p> <pre><code>pip freeze\n</code></pre> <p>to see which packages you have and which versions. You can run the following commands in the anaconda prompt to get the necessary packages:</p> <p><pre><code>pip install tensorflow  \npip install tensorboard\n</code></pre> (If you have an older version of tensorflow, you can upgrade with <code>pip install tensorflow --upgrade</code>).</p> <p>In addition to this, Tensorflow 2 requires you to install this visual studio package: https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads (if you don't have windows or would like to read more, you can do so here: https://www.tensorflow.org/install/pip).</p>"},{"location":"09%20Introduction%20to%20neural%20networks/#topics","title":"Topics","text":"<p>This lecture covers the fundamental aspects of\u00a0neural networks.\u00a0</p> <p>After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:</p> <ul> <li>Explain what is meant by artificial neurons, and how these are linked to form artificial neural networks.</li> <li>Explain what a perceptron is, and how it transforms an input vector to an output, including the importance of weights and biases.</li> <li>Discuss how to structure a neural network.</li> <li>Discuss and summarize the method of (stochastic) gradient descent and how it is used to train a neural network, including how the learning rate influences results.</li> <li>Reflect upon the problems caused by using (a) perceptrons as artificial neurons and (b) the accuracy as the metric to optimize during model training, including how these problems can be solved using activation and loss functions, respectively.</li> <li>Sketch different activation functions, including the sigmoid, tanh and ReLU functions, and discuss why the softmax activation function is generally used in the output layer.</li> <li>Implement a neural network in python using the tensorflow.keras module.</li> </ul>"},{"location":"10%20Dimensionality%20reduction/","title":"Index","text":"09 Dimensionality Reduction: PCA, LDA and tSNE"},{"location":"10%20Dimensionality%20reduction/#material","title":"Material:","text":"<p>Ch 8</p> <p>Session Notes X</p> <p>Session Notes Y</p> <p>Session material</p> <p>Steve Brunton has made a whole lecture series about the SVD. This is overkill but maybe check out the the Overview and the videoes about PCA.</p> <p>For a very appealing and visual explanation of SVD, you should take a look at Visual Kernel's video on the topic.</p> <p>Useful Resources on t-SNE:</p> <p>How to Use t-SNE Effectively</p> <p>openTSNE</p> <p>Documentation</p> <p>I've also added the original research papers leading up to t-SNE (not part of syllabus, just there for reference)</p> <p>If you want a really in depth introduction to t-SNE, look here</p> <p>And as always, Alexander Ihler is gold.</p>"},{"location":"10%20Dimensionality%20reduction/#topics","title":"Topics","text":"<p>This lecture covers unsupervised machine learning algorithms. We discuss how these can be used for dimensionality reduction. We cover the following algorithms:</p> <ul> <li>Principal component analysis (PCA)</li> <li>t-distributed stochastic neighbor embedding (t-SNE)</li> </ul> <p>After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:</p> <ul> <li>Use principle component analysis (PCA) to reduce the dimensions of your dataset</li> <li>Describe how PCA can be used for clustering analyses</li> <li>Create 2-dimensional clustering-plots in python using PCA and t-SNE</li> </ul> <p>If you missed the session in linear algebra, I recommend checking out some of the resources mentioned above.</p>"},{"location":"11%20Clustering/","title":"Index","text":"10 Clustering"},{"location":"11%20Clustering/#material","title":"Material:","text":"<p>Ch 9</p> <p>Session material: In this folder</p>"},{"location":"11%20Clustering/#topics","title":"Topics","text":"<p>This lecture covers clustering algorithms. We cover the following algorithms:</p> <ul> <li>k means</li> <li>Hierarchical clustering</li> <li>DBSCAN</li> </ul> <p>After attending this lecture, reading the corresponding part of the book and doing exercises, I expect you to be able to:</p> <ul> <li>Describe the following clustering algorithms along with their advantages and disadvantages:</li> <li>k-Means</li> <li>Agglomerative clustering</li> <li>DBSCAN</li> <li>Apply the above clustering algorithms in Python</li> <li>Evaluate clustering algorithms (e.g. by inspecting the output in 2D-plots or in other ways inspecting which elements are clustered together).</li> <li>Use a dendrogram to determine the optimal number of clusters</li> </ul>"},{"location":"12%20Recap/","title":"Index","text":"12 Perspectives in Articifical Intelligence"},{"location":"12%20Recap/#material","title":"Material:","text":"<p>Session material: In this folder.</p>"},{"location":"12%20Recap/#topics","title":"Topics","text":"<p>This lecture touches upon advanced ML methods and speculates on the future of AI.</p> <p>After attending this lecture, I expect you to be able to:</p> <ul> <li>Outline the basics of advanced ML methods, including convolutional and recurrent neural networks, generative adversial networks and reinforcement learning.</li> <li>Reflect upon the importance of AI ethics.</li> <li>Reflect upon how AI is changing and may change society.</li> </ul>"},{"location":"Sessions/","title":"Sessions","text":"<p>Click on a session to the left to access a plan of a specific session and additional resources for that session.</p> Session Date Topic 00 Important Recap 01 4 Feb 12:45 \u2013 16:05 Introduction + Recap Probability + Stochastic Variables 02 11 Feb 12:45 \u2013 16:05 Discrete Random Variables 03 18 Feb 12:45 \u2013 16:05 Continuous Random Variables 04 25 Feb 12:45 \u2013 16:05 Normal and Exponential Distributions and Multivariate Random Variables 05 4 Mar 12:45 \u2013 16:05 Multivariate Random Variables Part 2 06 11 Mar 12:45 \u2013 16:05 Point Estimation, Sampling and Statistical Intervals 07 25 Mar 12:45 \u2013 16:05 Hypothesis Testing 08 1 Apr 12:45 \u2013 16:05 Regression 09 8 Apr 12:45 \u2013 16:05 Introduction to Stochastic Processes 10 22 Apr 12:45 \u2013 16:05 Markov Chains 11 29 Apr 12:45 \u2013 16:05 Recap and Exercises Markov Chains"},{"location":"blog/","title":"Blog","text":""},{"location":"pages/exam/","title":"Exam","text":"Exam"},{"location":"pages/exam/#exam-prerequisites","title":"Exam prerequisites:","text":"<p>None</p>"},{"location":"pages/exam/#exam-type","title":"Exam type","text":"<p>The exam has two parts:</p> <ul> <li>The first part is a Flowlock exam in Wiseflow.</li> <li>The second part is a Wiseflow exam without Flowlock. The second part must be completed in the Jupyter Notebook environment and the answers must be submitted in Wiseflow.</li> </ul> <p>Part 1 has a duration of 3 hours and part 2 has a duration of 1 hour. The exam has a total duration of 4 hours.  The student will not be able to access the second part before the first part is concluded. Part 1 weighs 75% and Part 2 weighs 25% in the final grade.</p>"},{"location":"pages/exam/#tools-allowed","title":"Tools allowed","text":"<p>In the first part the students are allowed to use any notes, books, and/or other written/printed material and will have access to PDF files on their laptop.</p> <p>The students may bring their own calculator.</p> <p>In the second part all supplementary materials and aids are allowed, e.g., using a computer as a reference work.</p> <p>It is not allowed, however, to use AI-tools such as Copilot, ChatGPT, Bing, etc. Communication of any sort is not allowed during the exam and will lead to expulsion of all involved parties from the exam.</p>"},{"location":"pages/exam/#re-exam","title":"Re-exam","text":"<p>Re-exams may be oral.</p>"},{"location":"pages/faq/","title":"FAQ","text":"FAQ"},{"location":"pages/faq/#general-information","title":"General information","text":"Is there a FAQ for the course? <p>Yes, you are looking at it! This FAQ is designed to answer some of the most common questions about the course. If you have a question that is not answered here, please feel free to contact the course responsible, Richard Brooks</p> What is the course about? <p>The course covers an introduction to probability theory and statistics. In depth description can be found in the course description or by going the description of each session in the Sessions menu.</p> How is the course related to the study program? <p>The course mostly relates to the study program by providing a foundation for understanding and applying probability theory and statistics in the context of engineering, especially in the field of data science and machine learning.</p> What are the prerequisites for the course? <p>In the menu to the left, you can find the prerequisites for the course under the \"Prerequisites\" section. The course is designed to be self-contained, but it is recommended that students have a basic understanding of the topics listed.</p> Who should take this course? <p>The course is intended for students who are interested in learning about probability theory and statistics and how to apply them in the context of engineering. It is very useful for students who are interested in data science and machine learning. It is a complex course, and you should only take it if you are willing to put in the effort to learn the material. The course is mandatory for some Master's programs.</p> Is attendance mandatory? <p>Attendance is not mandatory, but it is highly recommended. The course is complex, and it is important to keep up with the material. If you are unable to attend a session, you should make sure to catch up on the material, e.g. by watching the recording of the 2021 session.</p>"},{"location":"pages/faq/#who-to-contact","title":"Who to contact?","text":"Who should I contact if I have questions about the course content? <p>You can contact the course responsible, Richard Brooks, if you have questions about the course content.</p> Who should I contact if I have questions about the exam? <p>You should always contact the Study Service if you have questions about the exam.</p> Who should I contact if I have questions about the schedule? <p>You can contact our scheduler if you have questions about the schedule.</p> Who should I contact if I have scheduling conflicts? <p>You can contact our scheduler if you have questions about scheduling conflicts.</p> Who should I contact if I want to know whether this course is mandatory for a Master's program or if it satisfies a specific requirement? <p>You can contact the Study Councillor.</p>"},{"location":"pages/faq/#exam-and-assessment","title":"Exam and assessment","text":"What type of exam will conclude the course <p>Please see the \"Exam\" section in the menu to the left for detailed information about the exam.</p> When will the exam and re-exam be held? <p>The exam is usually held in June and the re-exam in August. The exact dates will be published in the exam plan in MyVia. Feel free to contact the Study Service for more information.</p> What is the grading scale for the course? <p>The grading scale for the course is the 7-point grading scale.</p> How is the final grade calculated? <p>The final grade is calculated based on the exam. The exam has two parts: a Flowlock exam in Wiseflow and a Wiseflow exam without Flowlock. Part 1 weighs 75% and Part 2 weighs 25% in the final grade. Please see the \"Exam\" section in the menu to the left for more information.</p> What is the re-exam procedure? <p>Re-exams may be oral. Please see the \"Exam\" section in the menu to the left for more information.</p> What happens if I fail the exam? <p>If you fail the exam, you will have the opportunity to take a re-exam. Please see the \"Exam\" section in the menu to the left for more information.</p> What happens if I fail the re-exam? <p>If you fail the re-exam, you will have to wait until the course is held again to retake the exam. In special cases, we may be able to offer you an oral re-exam. Please see the \"Exam\" section in the menu to the left for more information.</p> How many percentage points do I need to pass the exam? <p>The exam is graded on a 7-point grading scale. To pass the exam, you need a grade of 02 or higher. In order to obtain a 02, you need to score at least 50% of the total points on the exam. This score may vary from year to year, but is never higher than 50%.</p>"},{"location":"pages/faq/#resources","title":"Resources","text":"Is the course book mandatory? <p>The course book is not mandatory, but I do recommend finding some resources to help you understand the material. The course book is a good resource, but there are many other resources available online. I recommend Probability Course as a good resource.</p> Is Python mandatory? <p>Python is mandatory for the course. You will need to use Python to complete the assignments and the exam. If you are not familiar with Python, I recommend finding some resources to help you learn the basics.</p> Is Jupyter Notebook mandatory? <p>Jupyter Notebook is mandatory for the course. You will need to use Jupyter Notebook to complete the assignments and the exam. You can install a Plugin in VSCode to run Jupyter Notebooks.</p> Is the course material available online? <p>Yes, the course material is available online. You can find all material by navigating the menu to the left.</p> Is there a recommended study plan? <p>Yes, I recommend following the study plan outlined in the course material. You can find the study plan in the course material by navigating the menu to the left.</p> Where do I find material such as old exam cases, solutions, and other resources? <p>You can find all material by navigating the menu to the left, see \"General Resources SMP\".</p> Are there any additional resources available? <p>Yes, there are many additional resources available online. I recommend checking out the resources listed in the course material. You can find the resources by navigating the menu to the left.</p> What is the Wiseflow code? <p>The Wiseflow code for the course is 0000. However, at the exam you will be given specific codes.</p>"}]}